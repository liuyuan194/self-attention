import torch
import torch.nn as nn

# ========================== #
# 1️⃣ Scaled Dot-Product Attention
# ========================== #
def scaled_dot_product_attention(q, k, v):
    """
    计算 Scaled Dot-Product Attention
    q, k, v: 形状 (B, num_heads, T, head_dim)
    """
    # 计算 QK^T / sqrt(head_dim)
    scores = torch.matmul(q, k.transpose(-2, -1)) / (q.shape[-1] ** 0.5)
    
    # 计算 Softmax 权重
    weights = scores.softmax(dim=-1)

    # 乘以 V 得到最终注意力输出
    return torch.matmul(weights, v)

# ========================== #
# 2️⃣ Multi-Head Attention
# ========================== #
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        """
        多头注意力层
        embed_dim: 词嵌入维度
        num_heads: 头的数量
        """
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads  # 每个头的维度

        # 线性变换，生成 Q, K, V
        self.qkv = nn.Linear(embed_dim, embed_dim * 3)

        # 最终输出层
        self.fc_out = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        """
        x: 输入形状 (B, T, embed_dim)
        """
        B, T, C = x.shape  # 获取 batch_size, 序列长度, 词嵌入维度

        # 计算 Q, K, V，并拆分
        qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # 变换维度 (3, B, num_heads, T, head_dim)
        q, k, v = qkv[0], qkv[1], qkv[2]  # 获取 Q, K, V

        # 计算注意力
        attn_output = scaled_dot_product_attention(q, k, v)

        # 变换维度，合并多头
        attn_output = attn_output.transpose(1, 2).reshape(B, T, C)

        # 通过最终的线性层
        return self.fc_out(attn_output)

# ========================== #
# 3️⃣ Transformer Block
# ========================== #
class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_hidden_dim):
        """
        Transformer 层
        embed_dim: 词嵌入维度
        num_heads: 头的数量
        ff_hidden_dim: Feed Forward 层的隐藏维度
        """
        super().__init__()
        self.attn = MultiHeadAttention(embed_dim, num_heads)  # 多头注意力
        self.norm1 = nn.LayerNorm(embed_dim)  # 归一化层 1

        # Feed Forward 网络
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_hidden_dim),
            nn.ReLU(),
            nn.Linear(ff_hidden_dim, embed_dim)
        )

        self.norm2 = nn.LayerNorm(embed_dim)  # 归一化层 2

    def forward(self, x):
        """
        x: 输入形状 (B, T, embed_dim)
        """
        # 计算注意力，并加上 Residual 连接
        attn_output = self.attn(x)
        x = self.norm1(x + attn_output)  # Add & Norm（跳跃连接）

        # 通过 Feed Forward 层，并加上 Residual 连接
        ff_output = self.ff(x)
        x = self.norm2(x + ff_output)  # Add & Norm（跳跃连接）

        return x

# ========================== #
# 4️⃣ GPT 模型
# ========================== #
class GPT(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_hidden_dim, max_length):
        """
        GPT 模型
        vocab_size: 词汇表大小
        embed_dim: 词嵌入维度
        num_heads: 头的数量
        num_layers: Transformer 层的数量
        ff_hidden_dim: Feed Forward 层的隐藏维度
        max_length: 句子最大长度
        """
        super().__init__()

        # 词嵌入层
        self.embed = nn.Embedding(vocab_size, embed_dim)

        # 位置编码（让模型知道单词的顺序）
        self.pos_embed = nn.Parameter(torch.randn(1, max_length, embed_dim))

        # Transformer 层（多个叠加）
        layers = []
        for _ in range(num_layers):
            layers.append(TransformerBlock(embed_dim, num_heads, ff_hidden_dim))
        self.layers = nn.ModuleList(layers)

        # 输出层，把隐藏状态转换成单词概率
        self.fc_out = nn.Linear(embed_dim, vocab_size)

    def forward(self, x):
        """
        x: 输入单词索引，形状 (B, T)
        """
        B, T = x.shape  # 获取 batch 大小和序列长度

        # 获取单词的词向量
        token_embedding = self.embed(x)

        # 获取位置编码，并与单词嵌入相加
        position_embedding = self.pos_embed[:, :T, :]
        x = token_embedding + position_embedding

        # 通过 Transformer 层
        for layer in self.layers:
            x = layer(x)

        # 计算最终输出
        return self.fc_out(x)

# ========================== #
# 5️⃣ 训练代码
# ========================== #
def train_gpt():
    """
    训练 GPT 模型
    """
    vocab_size = 50257
    embed_dim = 768
    num_heads = 12
    num_layers = 12
    ff_hidden_dim = 3072
    max_length = 512
    batch_size = 8
    epochs = 3
    lr = 3e-4

    model = GPT(vocab_size, embed_dim, num_heads, num_layers, ff_hidden_dim, max_length)
    dataset = torch.randint(0, vocab_size, (10000,))  # 生成随机数据
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    loss_fn = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        for batch in dataloader:
            optimizer.zero_grad()
            logits = model(batch.unsqueeze(0))  # 增加 batch 维度
            loss = loss_fn(logits.view(-1, vocab_size), batch.view(-1))
            loss.backward()
            optimizer.step()
        print(f"Epoch {epoch}, Loss: {loss.item()}")

    torch.save(model.state_dict(), "gpt_model.pth")
    print("模型训练完成，已保存！")

train_gpt()
